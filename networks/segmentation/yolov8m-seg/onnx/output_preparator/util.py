import cv2
import numpy

import torch
import torchvision


palette = {
    'background':   [0, 0, 0],
    'person':       [0, 128, 0],
    'bicycle':      [255, 153, 51],
    'car':          [51, 178, 255],
    'motorcycle':   [230, 230, 0],
    'airplane':     [255, 153, 255],
    'bus':          [153, 204, 255],
    'train':        [255, 102, 255],
    'truck':        [255, 51, 255],
    'boat':         [102, 178, 255],
    'traffic_light':[51, 153, 255],
    'fire_hydrant': [255, 153, 153],
    'stop_sign':    [255, 102, 102],
    'parking_meter':[255, 51, 51],
    'bench':        [153, 255, 153],
    'bird':         [102, 255, 102],
    'cat':          [51, 255, 51],
    'dog':          [0, 255, 0],
    'horse':        [0, 0, 255],
    "sheep":        [255, 0, 0],
    "cow":          [255, 255, 255],
    "elephant":     [153, 204, 255],
    "bear":         [255, 128, 0],
    "zebra":        [255, 153, 51],
    "giraffe":      [255, 178, 102],
    "backpack":     [230, 230, 0],
    "umbrella":     [255, 153, 255],
    "handbag":      [153, 204, 255],
    "tie":          [255, 102, 255],
    "suitcase":     [255, 51, 255],
    "frisbee":      [102, 178, 255],
    "skis":         [51, 153, 255],
    "snowboard":    [255, 153, 153],
    "sports_ball":  [255, 102, 102],
    "kite":         [255, 51, 51],
    "baseball_bat": [153, 255, 153],
    "baseball_glove": [102, 255, 102],
    "skateboard":   [51, 255, 51],
    "surfboard":    [0, 255, 0],
    "tennis_racket": [0, 0, 255],
    "bottle":       [255, 0, 0],
    "wine_glass":   [255, 255, 255],
    "cup":          [153, 204, 255],
    "fork":         [255, 128, 0],
    "knife":        [255, 153, 51],
    "spoon":        [255, 178, 102],
    "bowl":         [230, 230, 0],
    "banana":       [255, 153, 255],
    "apple":        [153, 204, 255],
    "sandwich":     [255, 102, 255],
    "orange":       [255, 51, 255],
    "broccoli":     [102, 178, 255],
    "carrot":       [51, 153, 255],
    "hot_dog":      [255, 153, 153],
    "pizza":        [255, 102, 102],
    "donut":        [255, 51, 51],
    "cake":         [153, 255, 153],
    "chair":        [102, 255, 102],
    "couch":        [51, 255, 51],
    "potted_plant": [0, 255, 0],
    "bed":          [0, 0, 255],
    "dining_table": [255, 0, 0],
    "toilet":       [255, 255, 255],
    "tv":           [0, 0, 255],
    "laptop":       [255, 128, 0],
    "mouse":        [255, 153, 51],
    "remote":       [255, 178, 102],
    "keyboard":     [230, 230, 0],
    "cell_phone":   [255, 153, 255],
    "microwave":    [153, 204, 255],
    "oven":         [255, 102, 255],
    "toaster":      [255, 51, 255],
    "sink":         [102, 178, 255],
    "refrigerator": [51, 153, 255],
    "book":         [255, 153, 153],
    "clock":        [255, 102, 102],
    "vase":         [255, 51, 51],
    "scissors":     [153, 255, 153],
    "teddy_bear":   [102, 255, 102],
    "hair_drier":   [51, 255, 51],
    "toothbrush":   [0, 255, 0]
}


def plot_box(x, img, color=None, label=None, line_thickness=None):
    # Plots one bounding box on image img
    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness
    color = color or [numpy.random.randint(0, 255) for _ in range(3)]
    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))
    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)
    if label:
        tf = max(tl - 1, 1)  # font thickness
        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]
        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3
        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled
        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)


def scale_coords(img1_shape, coords, img0_shape, letterbox=True):
    # Rescale coords (xyxy) from img1_shape to img0_shape
    if letterbox:
        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new
        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding
        coords[:, [0, 2]] -= pad[0]  # x padding
        coords[:, [1, 3]] -= pad[1]  # y padding
        coords[:, :4] /= gain
    else:
        gain = img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1]  # gain  = old / new
        pad = (img1_shape[1] - img0_shape[1] * gain[1]) / 2, (img1_shape[0] - img0_shape[0] * gain[0]) / 2  # wh padding
        coords[:, [0, 2]] -= pad[0]   # x padding
        coords[:, [1, 3]] -= pad[1]   # y padding
        coords[:, [0, 2]] /= gain[1]  # x1y1 scale
        coords[:, [1, 3]] /= gain[0]  # x2y2 scale
    clip_coords(coords, img0_shape)
    return coords


def clip_coords(boxes, img_shape):
    # Clip bounding xyxy bounding boxes to image shape (height, width)
    boxes[:, 0] = numpy.clip(boxes[:, 0], 0, img_shape[1])
    boxes[:, 1] = numpy.clip(boxes[:, 1], 0, img_shape[0])
    boxes[:, 2] = numpy.clip(boxes[:, 2], 0, img_shape[1])
    boxes[:, 3] = numpy.clip(boxes[:, 3], 0, img_shape[0])


def xywh2xyxy(x):
    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right
    y = numpy.zeros_like(x)
    dw = x[..., 2] / 2  # half-width
    dh = x[..., 3] / 2  # half-height
    y[..., 0] = x[..., 0] - dw  # top left x
    y[..., 1] = x[..., 1] - dh  # top left y
    y[..., 2] = x[..., 0] + dw  # bottom right x
    y[..., 3] = x[..., 1] + dh  # bottom right y
    return y


def box_iou(box1, box2):
    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py
    """
    Return intersection-over-union (Jaccard index) of boxes.
    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.
    Arguments:
        box1 (Tensor[N, 4])
        box2 (Tensor[M, 4])
    Returns:
        iou (Tensor[N, M]): the NxM matrix containing the pairwise
            IoU values for every element in boxes1 and boxes2
    """

    def box_area(box):
        # box = 4xn
        return (box[2] - box[0]) * (box[3] - box[1])

    area1 = box_area(box1.T)
    area2 = box_area(box2.T)

    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clip(0).prod(1)
    inter = (numpy.minimum(box1[2:4], box2[:, 2:4]) - numpy.maximum(box1[:2], box2[:, :2])).clip(0).prod(1)
    return inter / (area1 + area2 - inter)  # iou = inter / (area1 + area2 - inter)


def filter_out_boxes(prediction, conf_thres=0.1, iou_thres=0.6, nc=None, max_nms=1000, max_wh=7680):
    """ Performs Non-Maximum Suppression (NMS) on inference results
    Returns:
         detections with shape: nx6 (x1, y1, x2, y2, conf, cls)
    source: https://github.com/ultralytics/yolov3/blob/master/utils/general.py#L640
    """
    if prediction.dtype is numpy.float16:
        prediction = prediction.astype(numpy.float32)  # to FP32
    bs = prediction.shape[0]  # batch size
    nc = prediction.shape[1] - 4 if nc is None else nc # number of classes
    nm = prediction.shape[1] - nc - 4
    mi = 4 + nc  # mask start index
    xc = prediction[:, 4:mi].max(1) > conf_thres  # candidates

    # Settings
    output = [numpy.zeros((0, 6 + nm))]
    prediction = numpy.transpose(prediction, (0, 2, 1))
    prediction[..., :4] = xywh2xyxy(prediction[..., :4])  # xywh to xyxy

    max_det = 100  # maximum number of detections per image
    for xi, x in enumerate(prediction):  # image index, image inference
        # Apply constraints
        x = x[xc[xi]]  # confidence
        # If none remain process next image
        if not x.shape[0]:
            continue
        # Detections matrix nx6 (xyxy, conf, cls)
        box, cls, mask = x[:, :4], x[:, 4:nc+4], x[:, nc+4:]
        # Box (center x, center y, width, height) to (x1, y1, x2, y2)
        # box = xywh2xyxy(box)
        # Detections matrix nx6 (xyxy, conf, cls)
        conf = numpy.amax(cls, axis=1, keepdims=True)
        j = numpy.expand_dims(numpy.where(cls == conf)[1], axis=1)
        x = numpy.concatenate((box, conf, j.astype(numpy.float32), mask), axis=1)
        x = x[x[:, 4] > conf_thres]
        # If none remain process next image
        if not x.shape[0]:  # number of boxes
            continue
        # Batched NMS
        # x = x[-(x[:, 4]).argsort()[:max_nms]]
        c = x[:, 5:6] * max_wh  # classes
        boxes, scores = x[:, :4] + c, x[:, 4]
        output[xi] = x[torchvision_nms_kernel(boxes, scores, iou_thres)]
    return output


def torchvision_nms_kernel(dets, scores, iou_thr=0.4):
    """ https://github.com/pytorch/vision/blob/main/torchvision/csrc/ops/cpu/nms_kernel.cpp """

    x1 = dets[:, 0]
    y1 = dets[:, 1]
    x2 = dets[:, 2]
    y2 = dets[:, 3]

    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    order = scores.argsort()[::-1]

    keep = []
    while order.size > 0:
        i = order[0]
        keep.append(i)
        xx1 = numpy.maximum(x1[i], x1[order[1:]])
        yy1 = numpy.maximum(y1[i], y1[order[1:]])
        xx2 = numpy.minimum(x2[i], x2[order[1:]])
        yy2 = numpy.minimum(y2[i], y2[order[1:]])

        w = numpy.maximum(0.0, xx2 - xx1 + 1)  # maximum width
        h = numpy.maximum(0.0, yy2 - yy1 + 1)  # maximum height
        inter = w * h
        ovr = inter / (areas[i] + areas[order[1:]] - inter)

        inds = numpy.where(ovr <= iou_thr)[0]
        order = order[inds + 1]

    return keep


def nms(predictions, iou_threshold=0.4, max_nms=1000):
    """
    Removes detections with lower object confidence score than 'conf_thres'
    Non-Maximum Suppression to further filter detections.
    Returns detections with shape:
        (x1, y1, x2, y2, object_conf, class_conf, class)
    source: https://github.com/ultralytics/yolov3/blob/fbf0014cd6053695de7c732c42c7748293fb776f/utils/utils.py#L324
    with nms_style = 'OR' (default)
    """
    det_max = []
    # predictions = predictions[(-predictions[:, 4]).argsort()][:max_nms]
    for c in numpy.unique(predictions[:, -1]):
        dc = predictions[predictions[:, -1] == c]  # select class c
        n = len(dc)
        if n == 1:
            det_max.append(dc[0])  # No NMS required if only 1 prediction
            continue
        # Non-maximum suppression (OR)
        while dc.shape[0]:
            if len(dc.shape) > 1:  # Stop if we're at the last detection
                det_max.append(dc[0])  # save highest conf detection
            else:
                break
            iou = box_iou(dc[0], dc[1:])  # iou with other boxes
            dc = dc[1:][iou < iou_threshold]  # remove ious > threshold
    return numpy.array(det_max)

def non_max_suppression(
    prediction,
    conf_thres=0.25,
    iou_thres=0.45,
    classes=None,
    agnostic=False,
    multi_label=False,
    labels=(),
    max_det=300,
    nc=0,  # number of classes (optional)
    max_time_img=0.05,
    max_nms=30000,
    max_wh=7680,
    in_place=True,
    rotated=False,
):
    """
    Perform non-maximum suppression (NMS) on a set of boxes, with support for masks and multiple labels per box.

    Args:
        prediction (torch.Tensor): A tensor of shape (batch_size, num_classes + 4 + num_masks, num_boxes)
            containing the predicted boxes, classes, and masks. The tensor should be in the format
            output by a model, such as YOLO.
        conf_thres (float): The confidence threshold below which boxes will be filtered out.
            Valid values are between 0.0 and 1.0.
        iou_thres (float): The IoU threshold below which boxes will be filtered out during NMS.
            Valid values are between 0.0 and 1.0.
        classes (List[int]): A list of class indices to consider. If None, all classes will be considered.
        agnostic (bool): If True, the model is agnostic to the number of classes, and all
            classes will be considered as one.
        multi_label (bool): If True, each box may have multiple labels.
        labels (List[List[Union[int, float, torch.Tensor]]]): A list of lists, where each inner
            list contains the apriori labels for a given image. The list should be in the format
            output by a dataloader, with each label being a tuple of (class_index, x1, y1, x2, y2).
        max_det (int): The maximum number of boxes to keep after NMS.
        nc (int, optional): The number of classes output by the model. Any indices after this will be considered masks.
        max_time_img (float): The maximum time (seconds) for processing one image.
        max_nms (int): The maximum number of boxes into torchvision.ops.nms().
        max_wh (int): The maximum box width and height in pixels.
        in_place (bool): If True, the input prediction tensor will be modified in place.

    Returns:
        (List[torch.Tensor]): A list of length batch_size, where each element is a tensor of
            shape (num_boxes, 6 + num_masks) containing the kept boxes, with columns
            (x1, y1, x2, y2, confidence, class, mask1, mask2, ...).
    """

    # Checks
    assert 0 <= conf_thres <= 1, f"Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0"
    assert 0 <= iou_thres <= 1, f"Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0"
    if isinstance(prediction, (list, tuple)):  # YOLOv8 model in validation model, output = (inference_out, loss_out)
        prediction = prediction[0]  # select only inference output
    prediction = torch.Tensor(prediction)
    bs = prediction.shape[0]  # batch size
    nc = nc or (prediction.shape[1] - 4)  # number of classes
    nm = prediction.shape[1] - nc - 4
    mi = 4 + nc  # mask start index
    xc = prediction[:, 4:mi].amax(1) > conf_thres  # candidates

    # Settings
    # min_wh = 2  # (pixels) minimum box width and height
    time_limit = 2.0 + max_time_img * bs  # seconds to quit after
    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)

    prediction = prediction.transpose(-1, -2)  # shape(1,84,6300) to shape(1,6300,84)
    prediction[..., :4] = torch_xywh2xyxy(prediction[..., :4])  # xywh to xyxy

    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs
    for xi, x in enumerate(prediction):  # image index, image inference
        # Apply constraints
        # x[((x[:, 2:4] < min_wh) | (x[:, 2:4] > max_wh)).any(1), 4] = 0  # width-height
        x = x[xc[xi]]  # confidence

        # If none remain process next image
        if not x.shape[0]:
            continue

        # Detections matrix nx6 (xyxy, conf, cls)
        box, cls, mask = x.split((4, nc, nm), 1)

        if multi_label:
            i, j = torch.where(cls > conf_thres)
            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float(), mask[i]), 1)
        else:  # best class only
            conf, j = cls.max(1, keepdim=True)
            x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]

        # Filter by class
        if classes is not None:
            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]

        # Check shape
        n = x.shape[0]  # number of boxes
        if not n:  # no boxes
            continue
        if n > max_nms:  # excess boxes
            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes

        # Batched NMS
        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes
        scores = x[:, 4]  # scores
        boxes = x[:, :4] + c  # boxes (offset by class)
        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS
        i = i[:max_det]  # limit detections

        output[xi] = x[i]
    return output


def process_mask(protos, masks_in, bboxes, shape, upsample=False):
    """
    Apply masks to bounding boxes using the output of the mask head.

    Args:
        protos (torch.Tensor): A tensor of shape [mask_dim, mask_h, mask_w].
        masks_in (torch.Tensor): A tensor of shape [n, mask_dim], where n is the number of masks after NMS.
        bboxes (torch.Tensor): A tensor of shape [n, 4], where n is the number of masks after NMS.
        shape (tuple): A tuple of integers representing the size of the input image in the format (h, w).
        upsample (bool): A flag to indicate whether to upsample the mask to the original image size. Default is False.

    Returns:
        (torch.Tensor): A binary mask tensor of shape [n, h, w], where n is the number of masks after NMS, and h and w
            are the height and width of the input image. The mask is applied to the bounding boxes.
    """

    protos = torch.Tensor(protos)
    masks_in = torch.Tensor(masks_in)
    c, mh, mw = protos.shape  # CHW
    ih, iw = shape
    masks = (masks_in @ protos.float().view(c, -1)).sigmoid().view(-1, mh, mw)  # CHW
    width_ratio = mw / iw
    height_ratio = mh / ih

    downsampled_bboxes = bboxes.clone()
    downsampled_bboxes[:, 0] *= width_ratio
    downsampled_bboxes[:, 2] *= width_ratio
    downsampled_bboxes[:, 3] *= height_ratio
    downsampled_bboxes[:, 1] *= height_ratio

    masks = crop_mask(masks, downsampled_bboxes)  # CHW
    if upsample:
        masks = torch.nn.functional.interpolate(masks[None], shape, mode="bilinear", align_corners=False)[0]  # CHW
    return masks.gt_(0.5)


def crop_mask(masks, boxes):
    """
    It takes a mask and a bounding box, and returns a mask that is cropped to the bounding box.

    Args:
        masks (torch.Tensor): [n, h, w] tensor of masks
        boxes (torch.Tensor): [n, 4] tensor of bbox coordinates in relative point form

    Returns:
        (torch.Tensor): The masks are being cropped to the bounding box.
    """
    _, h, w = masks.shape
    x1, y1, x2, y2 = torch.chunk(boxes[:, :, None], 4, 1)  # x1 shape(n,1,1)
    r = torch.arange(w, device=masks.device, dtype=x1.dtype)[None, None, :]  # rows shape(1,1,w)
    c = torch.arange(h, device=masks.device, dtype=x1.dtype)[None, :, None]  # cols shape(1,h,1)

    return masks * ((r >= x1) * (r < x2) * (c >= y1) * (c < y2))


def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None, padding=True, xywh=False):
    """
    Rescales bounding boxes (in the format of xyxy by default) from the shape of the image they were originally
    specified in (img1_shape) to the shape of a different image (img0_shape).

    Args:
        img1_shape (tuple): The shape of the image that the bounding boxes are for, in the format of (height, width).
        boxes (torch.Tensor): the bounding boxes of the objects in the image, in the format of (x1, y1, x2, y2)
        img0_shape (tuple): the shape of the target image, in the format of (height, width).
        ratio_pad (tuple): a tuple of (ratio, pad) for scaling the boxes. If not provided, the ratio and pad will be
            calculated based on the size difference between the two images.
        padding (bool): If True, assuming the boxes is based on image augmented by yolo style. If False then do regular
            rescaling.
        xywh (bool): The box format is xywh or not, default=False.

    Returns:
        boxes (torch.Tensor): The scaled bounding boxes, in the format of (x1, y1, x2, y2)
    """
    if ratio_pad is None:  # calculate from img0_shape
        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new
        pad = (
            round((img1_shape[1] - img0_shape[1] * gain) / 2 - 0.1),
            round((img1_shape[0] - img0_shape[0] * gain) / 2 - 0.1),
        )  # wh padding
    else:
        gain = ratio_pad[0][0]
        pad = ratio_pad[1]

    if padding:
        boxes[..., 0] -= pad[0]  # x padding
        boxes[..., 1] -= pad[1]  # y padding
        if not xywh:
            boxes[..., 2] -= pad[0]  # x padding
            boxes[..., 3] -= pad[1]  # y padding
    boxes[..., :4] /= gain
    return clip_boxes(boxes, img0_shape)


def clip_boxes(boxes, shape):
    """
    Takes a list of bounding boxes and a shape (height, width) and clips the bounding boxes to the shape.

    Args:
        boxes (torch.Tensor): the bounding boxes to clip
        shape (tuple): the shape of the image

    Returns:
        (torch.Tensor | numpy.ndarray): Clipped boxes
    """
    if isinstance(boxes, torch.Tensor):  # faster individually (WARNING: inplace .clamp_() Apple MPS bug)
        boxes[..., 0] = boxes[..., 0].clamp(0, shape[1])  # x1
        boxes[..., 1] = boxes[..., 1].clamp(0, shape[0])  # y1
        boxes[..., 2] = boxes[..., 2].clamp(0, shape[1])  # x2
        boxes[..., 3] = boxes[..., 3].clamp(0, shape[0])  # y2
    else:  # np.array (faster grouped)
        boxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1])  # x1, x2
        boxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])  # y1, y2
    return boxes

def torch_xywh2xyxy(x):
    """
    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the
    top-left corner and (x2, y2) is the bottom-right corner.

    Args:
        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.

    Returns:
        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.
    """
    assert x.shape[-1] == 4, f"input shape last dimension expected 4 but input shape is {x.shape}"
    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy
    dw = x[..., 2] / 2  # half-width
    dh = x[..., 3] / 2  # half-height
    y[..., 0] = x[..., 0] - dw  # top left x
    y[..., 1] = x[..., 1] - dh  # top left y
    y[..., 2] = x[..., 0] + dw  # bottom right x
    y[..., 3] = x[..., 1] + dh  # bottom right y
    return y